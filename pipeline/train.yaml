# PIPELINE DEFINITION
# Name: train
# Inputs:
#    batch_size: int [Default: 2.0]
#    enable_caching: bool [Default: False]
#    epochs: int [Default: 2.0]
#    experiment_name: str [Default: 'uno-cards-v1.2-1']
#    experiments_root_folder: str [Default: 'experiments']
#    force_clean: bool [Default: False]
#    image_size: int [Default: 640.0]
#    images_dataset_name: str [Default: 'uno-cards-v1.2']
#    images_dataset_pvc_name: str [Default: 'images-datasets-pvc']
#    images_dataset_pvc_size_in_gi: int [Default: 5.0]
#    images_dataset_yaml: str [Default: 'data.yaml']
#    images_datasets_root_folder: str [Default: 'datasets']
#    map75_threshold: float [Default: 0.9]
#    model_name: str [Default: 'yolov8n']
#    models_root_folder: str [Default: 'models']
#    n_trials: int [Default: 1.0]
#    run_name: str [Default: 'uno-cards']
#    tracking_uri: str [Default: 'http://mlflow-server:8080']
# Outputs:
#    train-model-optuna-results_output_metrics: system.Metrics
components:
  comp-get-images-dataset:
    executorLabel: exec-get-images-dataset
    inputDefinitions:
      parameters:
        force_clean:
          parameterType: BOOLEAN
        images_dataset_name:
          parameterType: STRING
        images_dataset_yaml:
          parameterType: STRING
        images_datasets_root_folder:
          parameterType: STRING
        root_mount_path:
          parameterType: STRING
  comp-setup-storage:
    executorLabel: exec-setup-storage
    inputDefinitions:
      parameters:
        access_mode:
          defaultValue: ReadWriteOnce
          isOptional: true
          parameterType: STRING
        pvc_name:
          description: Name of the PVC to create.
          parameterType: STRING
        size_in_gi:
          description: Size of the PVC in GiB.
          parameterType: NUMBER_INTEGER
        storage_class:
          defaultValue: ''
          description: Storage class for the PVC. Default is an empty string.
          isOptional: true
          parameterType: STRING
  comp-train-model-optuna:
    executorLabel: exec-train-model-optuna
    inputDefinitions:
      parameters:
        epochs:
          parameterType: NUMBER_INTEGER
        experiment_name:
          parameterType: STRING
        experiments_root_folder:
          parameterType: STRING
        images_dataset_name:
          parameterType: STRING
        images_dataset_yaml:
          parameterType: STRING
        images_datasets_root_folder:
          parameterType: STRING
        model_name:
          parameterType: STRING
        models_root_folder:
          parameterType: STRING
        n_trials:
          parameterType: NUMBER_INTEGER
        root_mount_path:
          parameterType: STRING
        run_name:
          parameterType: STRING
        tracking_uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        results_output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        model_name_output:
          parameterType: STRING
  comp-upload-model:
    executorLabel: exec-upload-model
    inputDefinitions:
      parameters:
        model_name:
          parameterType: STRING
        models_root_folder:
          parameterType: STRING
        root_mount_path:
          parameterType: STRING
  comp-validate-metrics:
    executorLabel: exec-validate-metrics
    inputDefinitions:
      artifacts:
        metrics_input:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        thresholds:
          parameterType: STRUCT
deploymentSpec:
  executors:
    exec-get-images-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_images_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'boto3' 'botocore'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_images_dataset(\n    images_datasets_root_folder: str, \n\
          \    images_dataset_name: str,\n    images_dataset_yaml: str,\n    root_mount_path:\
          \ str,\n    force_clean: bool\n):\n    import boto3\n    import botocore\n\
          \    import os\n    import shutil\n    import re\n\n    aws_access_key_id\
          \ = os.environ.get('AWS_ACCESS_KEY_ID')\n    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n\
          \    endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n    region_name =\
          \ os.environ.get('AWS_DEFAULT_REGION')\n    bucket_name = os.environ.get('AWS_S3_BUCKET')\n\
          \n    # Construct and set the IMAGES_DATASET_S3_KEY environment variable\n\
          \    images_dataset_s3_key = f\"{images_datasets_root_folder}/{images_dataset_name}.zip\"\
          \n\n    print(f\"images_dataset_s3_key = {images_dataset_s3_key}\")\n\n\
          \    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n\
          \        aws_secret_access_key=aws_secret_access_key\n    )\n\n    s3_resource\
          \ = session.resource(\n        's3',\n        config=botocore.client.Config(signature_version='s3v4'),\n\
          \        endpoint_url=endpoint_url,\n        region_name=region_name\n \
          \   )\n\n    bucket = s3_resource.Bucket(bucket_name)\n\n    # Create a\
          \ temporary directory to store the dataset\n    local_tmp_dir = '/tmp/get_images_dataset'\n\
          \    print(f\">>> local_tmp_dir: {local_tmp_dir}\")\n\n    # Ensure local_tmp_dir\
          \ exists\n    if not os.path.exists(local_tmp_dir):\n        os.makedirs(local_tmp_dir)\n\
          \n    # Get the file name from the S3 key\n    file_name = f\"{images_dataset_name}.zip\"\
          \n    # Download the file\n    local_file_path = f'{local_tmp_dir}/{file_name}'\n\
          \n    # If file doesn't exist in the bucket raise a ValueError\n    objs\
          \ = list(bucket.objects.filter(Prefix=images_dataset_s3_key))\n    if not\
          \ any(obj.key == images_dataset_s3_key for obj in objs):\n        raise\
          \ ValueError(f\"File {images_dataset_s3_key} does not exist in the bucket\
          \ {bucket_name}\")\n\n    print(f\"Downloading {images_dataset_s3_key} to\
          \ {local_file_path}\")\n    bucket.download_file(images_dataset_s3_key,\
          \ local_file_path)\n    print(f\"Downloaded {images_dataset_s3_key}\")\n\
          \n    # Dataset path\n    images_dataset_path = f\"{root_mount_path}/{images_datasets_root_folder}\"\
          \n\n    # Ensure dataset path exists\n    if not os.path.exists(images_dataset_path):\n\
          \        os.makedirs(images_dataset_path)\n\n    # List the files in the\
          \ dataset path\n    print(f\"Listing files in {images_dataset_path}\")\n\
          \    print(os.listdir(images_dataset_path))\n\n    # If we haven't unzipped\
          \ the file yet or we're forced to, unzip it\n    images_dataset_folder =\
          \ f\"{images_dataset_path}/{images_dataset_name}\"\n    if not os.path.exists(images_dataset_folder)\
          \ or force_clean:\n        # Unzip the file into the images dataset volume\
          \ mount path\n        print(f\"Unzipping {local_file_path} to {images_dataset_path}\"\
          )\n        shutil.unpack_archive(f'{local_file_path}', f'{images_dataset_path}')\n\
          \        print(f\"Unzipped {local_file_path} to {images_dataset_path}\"\
          )\n\n        # List the files inside images_dataset_folder folder\n    \
          \    print(f\"Listing files in {images_dataset_folder}\")\n        print(os.listdir(images_dataset_folder))\n\
          \n    # Locate the YAML file in the dataset folder and replace the path\
          \ with the actual path\n    images_dataset_yaml_path = os.path.join(images_dataset_folder,\
          \ images_dataset_yaml)\n    print(f\"images_dataset_yaml_path: {images_dataset_yaml_path}\"\
          )\n\n    # If the YAML file doesn't exist, raise a ValueError\n    if not\
          \ os.path.exists(images_dataset_yaml_path):\n        raise ValueError(f\"\
          Dataset YAML file {images_dataset_yaml} not found in {images_dataset_folder}\"\
          )\n\n    # Replace regex 'path: .*' with 'path: {images_dataset_folder}'\n\
          \    with open(images_dataset_yaml_path, 'r') as f:\n        data = f.read()\n\
          \        data = re.sub(r'path: .*', f'path: {images_dataset_folder}', data)\n\
          \        # Print the updated YAML file\n        print(f\"Updated YAML file:\
          \ {data}\")\n        # Write the updated YAML file\n        with open(images_dataset_yaml_path,\
          \ 'w') as f:\n            f.write(data)\n\n"
        env:
        - name: IMAGES_DATASET_S3_KEY
          value: '{{channel:task=;name=images_datasets_root_folder;type=String;}}/{{channel:task=;name=images_dataset_name;type=String;}}.zip'
        image: quay.io/modh/runtime-images:runtime-pytorch-ubi9-python-3.9-20241111
    exec-setup-storage:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - setup_storage
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kubernetes==23.6.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef setup_storage(\n    pvc_name: str,\n    size_in_gi: int,\n  \
          \  access_mode: str = \"ReadWriteOnce\",\n    storage_class: str = \"\"\n\
          ) -> None:\n    \"\"\"Sets up a PersistentVolumeClaim (PVC) if it does not\
          \ exist.\n\n    Args:\n        pvc_name (str): Name of the PVC to create.\n\
          \        size_in_gi (int): Size of the PVC in GiB.\n        storage_class\
          \ (str): Storage class for the PVC. Default is an empty string.\n\n    Raises:\n\
          \        ValueError: If `size_in_gi` is less than 0.\n        RuntimeError:\
          \ If there's any other issue in PVC creation.\n    \"\"\"\n    import os\n\
          \    from kubernetes import client\n    from kubernetes.client.rest import\
          \ ApiException\n\n    if size_in_gi < 0:\n        raise ValueError(\"size_in_gi\
          \ must be a non-negative integer.\")\n\n    print(f\"Creating PVC '{pvc_name}'\
          \ with size {size_in_gi}Gi, access mode '{access_mode}', and storage class\
          \ '{storage_class}'.\") \n\n    # Create configuration using the service\
          \ account token and namespace\n    configuration = client.Configuration()\n\
          \n    # Load the service account token and namespace from the mounted paths\n\
          \    token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n\
          \    namespace_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n\
          \n    # Read the token\n    with open(token_path, 'r') as token_file:\n\
          \        token = token_file.read().strip()\n\n    # Read the namespace\n\
          \    with open(namespace_path, 'r') as namespace_file:\n        namespace\
          \ = namespace_file.read().strip()\n\n    print(f\"Token: {token} Namespace:\
          \ {namespace}\")\n\n    # Configure the client\n    # curl -k --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\
          \ \\\n    #   -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\"\
          \  \\\n    #   https://kubernetes.default.svc/api/v1/namespaces/iniciativa-2/persistentvolumeclaims/images-datasets-pvc\n\
          \    kubernetes_host = f\"https://{os.getenv('KUBERNETES_SERVICE_HOST',\
          \ 'kubernetes.default.svc')}:{os.getenv('KUBERNETES_SERVICE_PORT', '443')}\"\
          \n    print(f\"kubernetes_host: {kubernetes_host}\")\n    configuration.host\
          \ = kubernetes_host\n    # configuration.host = 'https://kubernetes.default.svc'\n\
          \    configuration.verify_ssl = True\n    configuration.ssl_ca_cert = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'\n\
          \    configuration.api_key['authorization'] = token\n    configuration.api_key_prefix['authorization']\
          \ = 'Bearer'\n\n    # Print all the configuration settings\n    print(\"\
          Configuration settings:\")\n    for attr, value in vars(configuration).items():\n\
          \        print(f\"{attr}: {value}\")\n\n    print(\"Configured Kubernetes\
          \ API Host:\", configuration.host)\n    # Create an API client with the\
          \ configuration\n    api_client = client.ApiClient(configuration)\n    print(\"\
          API Client Host:\", api_client.configuration.host)\n\n    # Use the CoreV1\
          \ API to list PVCs\n    v1 = client.CoreV1Api(api_client)\n\n    # Check\
          \ if the PVC already exists\n    try:\n        v1.read_namespaced_persistent_volume_claim(name=pvc_name,\
          \ namespace=namespace)\n        print(f\"PVC '{pvc_name}' already exists.\"\
          )\n        return\n    except ApiException as e:\n        if e.status !=\
          \ 404:\n            raise RuntimeError(f\"Error checking for existing PVC:\
          \ {e}\")\n\n    # Define PVC spec\n    pvc_spec = client.V1PersistentVolumeClaim(\n\
          \        metadata=client.V1ObjectMeta(name=pvc_name),\n        spec=client.V1PersistentVolumeClaimSpec(\n\
          \            access_modes=[access_mode],\n            resources=client.V1ResourceRequirements(\n\
          \                requests={\"storage\": f\"{size_in_gi}Gi\"}\n         \
          \   )\n        )\n    )\n\n    # Add storage class if provided\n    if storage_class:\n\
          \        pvc_spec.spec.storage_class_name = storage_class\n\n    # Attempt\
          \ to create the PVC\n    try:\n        v1.create_namespaced_persistent_volume_claim(namespace=namespace,\
          \ body=pvc_spec)\n        print(f\"PVC '{pvc_name}' created successfully\
          \ in namespace '{namespace}'.\")\n    except ApiException as e:\n      \
          \  raise RuntimeError(f\"Failed to create PVC: {e.reason}\")\n\n"
        image: quay.io/modh/runtime-images:runtime-pytorch-ubi9-python-3.9-20241111
    exec-train-model-optuna:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model_optuna
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'boto3==1.35.54'\
          \ 'botocore==1.35.54' 'ultralytics==8.3.22' 'load_dotenv==0.1.0' 'numpy==1.26.4'\
          \ 'mlflow==2.17.1' 'onnxruntime==1.19.2' 'onnxslim==0.1.36' 'optuna==4.1.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model_optuna(\n    model_name: str,                   \
          \ # e.g: yolov8n\n    n_trials: int,                      # e.g: 5\n   \
          \ epochs: int,                        # e.g: 2\n    experiment_name: str,\
          \               # e.g: yolo-uno-cards\n    run_name: str,              \
          \        # e.g: yolov-run-uno-cards\n    tracking_uri: str,            \
          \      # e.g: http://mlflow.kubeflow.svc.cluster.local:5000\n    images_dataset_name:\
          \ str,           # e.g: uno-cards-v1.2\n    images_datasets_root_folder:\
          \ str,   # e.g: /tmp/datasets\n    images_dataset_yaml: str,           #\
          \ e.g: data.yaml\n    models_root_folder: str,            # e.g: models\n\
          \    experiments_root_folder: str,            # e.g: experiments\n    root_mount_path:\
          \ str,               # e.g: /tmp/\n    model_name_output: OutputPath(str),\n\
          \    results_output_metrics: Output[Metrics]\n):\n    import os\n    import\
          \ shutil\n    import time\n\n    import torch\n    from ultralytics import\
          \ YOLO, settings\n    import mlflow\n\n    import numpy as np\n\n    import\
          \ boto3\n    import botocore\n\n    import yaml\n\n    import optuna\n\n\
          \    # Load search space from YAML\n    def load_search_space(file_path):\n\
          \        with open(file_path, \"r\") as f:\n            return yaml.safe_load(f)\n\
          \n    # Map metric to model name\n    models = {}\n\n    # Define the objective\
          \ function\n    def objective(model_name, dataset_path, trial, search_space):\n\
          \        # Dynamically define hyperparameter search space\n        params\
          \ = {}\n        for param_name, param_config in search_space.items():\n\
          \            param_type = param_config[\"type\"]\n            if param_type\
          \ == \"float\":\n                params[param_name] = trial.suggest_float(\n\
          \                    param_name, param_config[\"low\"], param_config[\"\
          high\"]\n                )\n            elif param_type == \"uniform\":\n\
          \                params[param_name] = trial.suggest_uniform(\n         \
          \           param_name, param_config[\"low\"], param_config[\"high\"]\n\
          \                )\n            elif param_type == \"categorical\":\n  \
          \              params[param_name] = trial.suggest_categorical(\n       \
          \             param_name, param_config[\"choices\"]\n                )\n\
          \            else:\n                raise ValueError(f\"Unsupported parameter\
          \ type: {param_type}\")\n\n        # Define the YOLO model and dataset path\n\
          \        # Load the model\n        model = YOLO(f'{model_name}.pt')\n\n\
          \        # Set the run name\n        train_run_name = f\"{run_name}-{int(time.time())}\"\
          \n        print(f\"Current run name: {train_run_name}\")\n\n        # Start\
          \ the MLflow run for training\n        with mlflow.start_run(run_name=train_run_name)\
          \ as training_mlrun:\n            mlflow.log_param(\"dataset_file\", f\"\
          {endpoint_url}/{bucket_name}/{images_dataset_s3_key}\")\n            mlflow.log_param(\"\
          dataset_name\", images_dataset_name)\n            mlflow.log_param(\"datasets_root_folder\"\
          , images_datasets_root_folder)\n            mlflow.log_param(\"dataset_yaml\"\
          , images_dataset_yaml)\n            mlflow.log_param(\"device\", device)\n\
          \            mlflow.log_param(\"batch_size\", params[\"batch_size\"])\n\
          \            mlflow.log_param(\"imgsz\", params[\"input_size\"])\n\n   \
          \         print(f\"Training model {model_name} with dataset {images_dataset_yaml_path}.\"\
          )\n            # Train the model with sampled hyperparameters\n        \
          \    results = model.train(\n                data=dataset_path,\n      \
          \          epochs=epochs,\n                imgsz=params[\"input_size\"],\n\
          \                batch=params[\"batch_size\"],\n                optimizer=params[\"\
          optimizer\"],\n                lr0=params[\"learning_rate\"],\n        \
          \        momentum=params[\"momentum\"],\n                weight_decay=params[\"\
          weight_decay\"],\n                conf=params[\"confidence_threshold\"],\n\
          \                iou=params[\"iou_threshold\"],\n                label_smoothing=params[\"\
          label_smoothing\"],\n                device=device\n            )\n\n  \
          \          # If the results have the box attribute, log the metrics\n  \
          \          if hasattr(results, 'box'):\n                results_output_metrics.log_metric(\"\
          training/map\", results.box.map)\n                results_output_metrics.log_metric(\"\
          training/map50\", results.box.map50)\n                results_output_metrics.log_metric(\"\
          training/map75\", results.box.map75)\n                results_output_metrics.log_metric(\"\
          training/mp\", results.box.mp)\n                results_output_metrics.log_metric(\"\
          training/mr\", results.box.mr)\n                results_output_metrics.log_metric(\"\
          training/nc\", results.box.nc)\n            else:\n                raise\
          \ ValueError(\"No box attribute in the results!!!\")\n\n            # Use\
          \ the evaluation metric as the objective\n            mAP = results.box.map\n\
          \n            # Save the trained model\n            print(f\"Saving model\
          \ to {models_folder}\")\n            trained_model_name = f\"{model_name}-{train_run_name}\"\
          \n            print(f\"Trained model name: {trained_model_name}\")\n   \
          \         trained_model_pt_path = os.path.join(models_folder, f\"{trained_model_name}.pt\"\
          )\n            print(f\"Saving model to {trained_model_pt_path}\")\n   \
          \         model.save(trained_model_pt_path)\n\n            # Save model\
          \ location by objective metric\n            models[trained_model_name] =\
          \ mAP\n\n            # End the run\n            mlflow.end_run()\n\n   \
          \         return mAP  # Higher mAP is better\n\n        if not training_mlrun:\n\
          \            raise ValueError(\"MLflow run was not started\")\n\n    # Set\
          \ the S3 bucket connection\n    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n\
          \    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n \
          \   endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n    region_name =\
          \ os.environ.get('AWS_DEFAULT_REGION')\n    bucket_name = os.environ.get('AWS_S3_BUCKET')\n\
          \    images_dataset_s3_key = os.environ.get('IMAGES_DATASET_S3_KEY')\n\n\
          \    print(f\"S3: endpoint_url {endpoint_url}\")\n    print(f\"S3: bucket_name\
          \ {bucket_name}\")\n    print(f\"S3: images_dataset_s3_key {images_dataset_s3_key}\"\
          )\n\n    print(f\"tracking_uri {tracking_uri}\")\n    print(f\"experiment_name\
          \ {experiment_name}\")\n    print(f\"images_dataset_name {images_dataset_name}\"\
          )\n    print(f\"images_datasets_root_folder {images_datasets_root_folder}\"\
          )\n    print(f\"images_dataset_yaml {images_dataset_yaml}\")\n    print(f\"\
          models_root_folder {models_root_folder}\")\n    print(f\"root_mount_path\
          \ {root_mount_path}\")\n\n    session = boto3.session.Session(\n       \
          \ aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n\
          \    )\n\n    if not endpoint_url:\n        s3_resource = session.resource(\n\
          \            's3',\n            config=botocore.client.Config(signature_version='s3v4'),\n\
          \            region_name=region_name\n        )\n    else:\n        s3_resource\
          \ = session.resource(\n            's3',\n            config=botocore.client.Config(signature_version='s3v4'),\n\
          \            endpoint_url=endpoint_url,\n            region_name=region_name\n\
          \        )\n\n    bucket = s3_resource.Bucket(bucket_name)\n\n    # Create\
          \ a temporary directory to store the dataset\n    local_tmp_dir = '/tmp/optuna'\n\
          \    print(f\">>> local_tmp_dir: {local_tmp_dir}\")\n\n    # Ensure local_tmp_dir\
          \ exists\n    if not os.path.exists(local_tmp_dir):\n        os.makedirs(local_tmp_dir)\n\
          \n    # Get the optuna study file name from the S3 key\n    study_name =\
          \ f\"{experiment_name}-optuna\"\n    study_file_name = f\"{study_name}.yaml\"\
          \n    study_file_key = f\"{experiments_root_folder}/{study_file_name}\"\n\
          \n    print(f\"study_file_name = {study_file_name}\")\n\n    # Local study\
          \ file path\n    study_local_file_path = f'{local_tmp_dir}/{study_file_name}'\n\
          \n    # If optuna study file doesn't exist in the bucket raise a ValueError\n\
          \    objs = list(bucket.objects.filter(Prefix=study_file_key))\n    if not\
          \ any(obj.key == study_file_key for obj in objs):\n        raise ValueError(f\"\
          File {study_file_key} does not exist in the bucket {bucket_name}\")\n\n\
          \    print(f\"Downloading {study_file_key} to {study_local_file_path}\"\
          )\n    bucket.download_file(study_file_key, study_local_file_path)\n   \
          \ print(f\"Downloaded {study_file_key}\")\n\n    # If root_mount_path is\
          \ not set or doesn't exist, raise a ValueError\n    if not root_mount_path\
          \ or not os.path.exists(root_mount_path):\n        raise ValueError(f\"\
          Root mount path '{root_mount_path}' does not exist\")\n\n    # Set the images\
          \ dataset folder\n    images_dataset_folder = os.path.join(root_mount_path,\
          \ images_datasets_root_folder, images_dataset_name)\n\n    # If the images\
          \ dataset folder doesn't exist, raise a ValueError\n    if not os.path.exists(images_dataset_folder):\n\
          \        raise ValueError(f\"Images dataset folder {images_dataset_folder}\
          \ does not exist\")\n\n    # Set the models folder\n    models_folder =\
          \ os.path.join(root_mount_path, models_root_folder)\n\n    # Make sure the\
          \ models folder exists\n    if not os.path.exists(models_folder):\n    \
          \    os.makedirs(models_folder)\n\n    # Set the images dataset YAML path\n\
          \    images_dataset_yaml_path = os.path.join(images_dataset_folder, images_dataset_yaml)\n\
          \    print(f\"Checking if {images_dataset_yaml_path} exists\")\n    if not\
          \ os.path.exists(images_dataset_yaml_path):\n        raise ValueError(f\"\
          Dataset YAML file {images_dataset_yaml} not found in {images_dataset_folder}\"\
          )\n    print(f\"Dataset YAML file found in {images_dataset_yaml_path}\"\
          )\n\n    # Set the MLflow tracking URI and experiment\n    os.environ[\"\
          MLFLOW_TRACKING_URI\"] = tracking_uri\n    os.environ[\"MLFLOW_EXPERIMENT_NAME\"\
          ] = experiment_name\n\n    # Update a setting\n    settings.update({\"mlflow\"\
          : True})\n\n    # Set device\n    device = \"cpu\"\n    if torch.cuda.is_available():\n\
          \        device = \"cuda\"\n        print(f\"Using device: {torch.cuda.get_device_name(0)}\"\
          )\n    elif torch.backends.mps.is_available():\n        device = \"mps\"\
          \n        print(\"Using device: MPS\")\n    print(f\"Using device: {device}\"\
          )\n\n    # Reset settings to default values\n    settings.reset()\n\n  \
          \  # Load the search space\n    search_space = load_search_space(study_local_file_path)\n\
          \n    # optuna storage\n    storage = f\"sqlite:///{local_tmp_dir}/optuna.db\"\
          \n\n    # Create or load the study\n    study = optuna.create_study(\n \
          \       direction=\"maximize\",\n        storage=storage,\n        study_name=study_name,\n\
          \        load_if_exists=True\n    )\n\n    # Run the optimization callback\
          \ a func\n    study.optimize(lambda trial: objective(model_name, images_dataset_yaml_path,\
          \ trial, search_space), \n                   n_trials=n_trials)\n\n    #\
          \ Print the best hyperparameters\n    print(\"\\nBest Hyperparameters:\"\
          )\n    print(study.best_params)\n\n    # Print study statistics\n    print(\"\
          \\nStudy best trial:\")\n    print(study.best_trial)\n    print(\"\\nStudy\
          \ best value:\")\n    print(study.best_value)\n    print(\"\\nStudy trials:\"\
          )\n    print(study.trials)\n\n    # Print models\n    print(\"\\nModels:\"\
          )\n    print(models)\n\n    # Find the best model by metric (greater is\
          \ better) in the models dictionary\n    best_model_name = max(models, key=models.get)\n\
          \    print(f\"Best model: {best_model_name}\")\n\n    # Load the best model\n\
          \    best_model_name_path = os.path.join(models_folder, f\"{best_model_name}.pt\"\
          )\n    best_model = YOLO(f'{best_model_name_path}')\n\n    # Export the\
          \ model to ONNX\n    trained_model_onnx_path_tmp = best_model.export(format=\"\
          onnx\")\n    print(f\"Exported model to {trained_model_onnx_path_tmp}\"\
          )\n    if not trained_model_onnx_path_tmp:\n        print(\"Failed to export\
          \ model to ONNX format\")\n\n    # Set the output paths\n    with open(model_name_output,\
          \ 'w') as f:\n        f.write(best_model_name)\n\n"
        env:
        - name: IMAGES_DATASET_S3_KEY
          value: '{{channel:task=;name=images_datasets_root_folder;type=String;}}/{{channel:task=;name=images_dataset_name;type=String;}}.zip'
        image: quay.io/modh/runtime-images:runtime-pytorch-ubi9-python-3.9-20241111
        resources:
          cpuLimit: 6.0
          cpuRequest: 6.0
          memoryLimit: 12.884901888
          memoryRequest: 12.884901888
    exec-upload-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'boto3' 'botocore'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model(\n    root_mount_path: str,\n    models_root_folder:\
          \ str,\n    model_name: str,\n    ):\n    import os\n    import boto3\n\
          \    import botocore\n\n    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n\
          \    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n \
          \   endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n    region_name =\
          \ os.environ.get('AWS_DEFAULT_REGION')\n    bucket_name = os.environ.get('AWS_S3_BUCKET')\n\
          \n    models_s3_key = os.environ.get(\"MODELS_S3_KEY\")\n\n    # Set the\
          \ models folder\n    models_folder = os.path.join(root_mount_path, models_root_folder)\n\
          \n    # Set the model paths for the model.pt and model.onnx\n    model_pt_path\
          \ = os.path.join(models_folder, f\"{model_name}.pt\")\n    model_onnx_path\
          \ = os.path.join(models_folder, f\"{model_name}.onnx\")\n\n    print(f\"\
          Uploading {model_pt_path} and {model_onnx_path} to {models_s3_key} in {bucket_name}\
          \ bucket in {endpoint_url} endpoint\")\n\n    session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n\
          \                                    aws_secret_access_key=aws_secret_access_key)\n\
          \n    s3_resource = session.resource(\n        's3',\n        config=botocore.client.Config(signature_version='s3v4'),\n\
          \        endpoint_url=endpoint_url,\n        region_name=region_name)\n\n\
          \    bucket = s3_resource.Bucket(bucket_name)\n\n    print(f\"Uploading\
          \ to {models_s3_key}\")\n\n    # Upload the model.pt and model.onnx files\
          \ to the S3 bucket\n    for model_path in [model_pt_path, model_onnx_path]:\n\
          \        if not os.path.exists(model_path):\n            raise ValueError(f\"\
          Model file {model_path} does not exist\")\n\n        # Upload the file\n\
          \        print(f\"Uploading {model_path} to {models_s3_key}/{os.path.basename(model_path)}\"\
          )\n        bucket.upload_file(model_path, f\"{models_s3_key}/{os.path.basename(model_path)}\"\
          )\n\n"
        env:
        - name: MODELS_S3_KEY
          value: models/yolo
        image: quay.io/modh/runtime-images:runtime-pytorch-ubi9-python-3.9-20241111
    exec-validate-metrics:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_metrics
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'onnx==1.16.1'\
          \ 'onnxruntime==1.18.0' 'scikit-learn==1.5.0' 'numpy==1.24.3' 'pandas==2.2.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef validate_metrics(thresholds: Dict[str, float], metrics_input:\
          \ Input[Metrics]):\n    print(f\"thresholds: {thresholds}\")\n\n    # For\
          \ each threshold, check if the metric is above the threshold\n    for metric_name,\
          \ threshold in thresholds.items():\n        print(f\"metric_name: {metric_name},\
          \ threshold: {threshold}\")\n        metric_value = float(metrics_input.metadata[metric_name])\n\
          \        print(f\"metric_value: {metric_value}\")\n        # Make sure metric_value\
          \ and threshold are floats\n        metric_value = float(metric_value)\n\
          \        threshold = float(threshold)\n        # If the metric is below\
          \ the threshold, raise a ValueError\n        if metric_value <= threshold:\n\
          \            raise ValueError(f\"{metric_name} is below the threshold of\
          \ {threshold}\")\n\n"
        image: quay.io/modh/runtime-images:runtime-pytorch-ubi9-python-3.9-20241111
pipelineInfo:
  name: train
root:
  dag:
    outputs:
      artifacts:
        train-model-optuna-results_output_metrics:
          artifactSelectors:
          - outputArtifactKey: results_output_metrics
            producerSubtask: train-model-optuna
    tasks:
      get-images-dataset:
        cachingOptions: {}
        componentRef:
          name: comp-get-images-dataset
        dependentTasks:
        - setup-storage
        inputs:
          parameters:
            force_clean:
              componentInputParameter: force_clean
            images_dataset_name:
              componentInputParameter: images_dataset_name
            images_dataset_yaml:
              componentInputParameter: images_dataset_yaml
            images_datasets_root_folder:
              componentInputParameter: images_datasets_root_folder
            root_mount_path:
              runtimeValue:
                constant: /opt/app-root/src
        taskInfo:
          name: get-images-dataset
      setup-storage:
        cachingOptions: {}
        componentRef:
          name: comp-setup-storage
        inputs:
          parameters:
            pvc_name:
              componentInputParameter: images_dataset_pvc_name
            size_in_gi:
              componentInputParameter: images_dataset_pvc_size_in_gi
        taskInfo:
          name: setup-storage
      train-model-optuna:
        cachingOptions: {}
        componentRef:
          name: comp-train-model-optuna
        dependentTasks:
        - get-images-dataset
        inputs:
          parameters:
            epochs:
              componentInputParameter: epochs
            experiment_name:
              componentInputParameter: experiment_name
            experiments_root_folder:
              componentInputParameter: experiments_root_folder
            images_dataset_name:
              componentInputParameter: images_dataset_name
            images_dataset_yaml:
              componentInputParameter: images_dataset_yaml
            images_datasets_root_folder:
              componentInputParameter: images_datasets_root_folder
            model_name:
              componentInputParameter: model_name
            models_root_folder:
              componentInputParameter: models_root_folder
            n_trials:
              componentInputParameter: n_trials
            root_mount_path:
              runtimeValue:
                constant: /opt/app-root/src
            run_name:
              componentInputParameter: run_name
            tracking_uri:
              componentInputParameter: tracking_uri
        taskInfo:
          name: train-model-optuna
      upload-model:
        cachingOptions: {}
        componentRef:
          name: comp-upload-model
        dependentTasks:
        - train-model-optuna
        - validate-metrics
        inputs:
          parameters:
            model_name:
              taskOutputParameter:
                outputParameterKey: model_name_output
                producerTask: train-model-optuna
            models_root_folder:
              componentInputParameter: models_root_folder
            root_mount_path:
              runtimeValue:
                constant: /opt/app-root/src
        taskInfo:
          name: upload-model
      validate-metrics:
        cachingOptions: {}
        componentRef:
          name: comp-validate-metrics
        dependentTasks:
        - train-model-optuna
        inputs:
          artifacts:
            metrics_input:
              taskOutputArtifact:
                outputArtifactKey: results_output_metrics
                producerTask: train-model-optuna
          parameters:
            pipelinechannel--map75_threshold:
              componentInputParameter: map75_threshold
            thresholds:
              runtimeValue:
                constant:
                  val/map75: '{{$.inputs.parameters[''pipelinechannel--map75_threshold'']}}'
        taskInfo:
          name: validate-metrics
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      enable_caching:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      epochs:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      experiment_name:
        defaultValue: uno-cards-v1.2-1
        isOptional: true
        parameterType: STRING
      experiments_root_folder:
        defaultValue: experiments
        isOptional: true
        parameterType: STRING
      force_clean:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      image_size:
        defaultValue: 640.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      images_dataset_name:
        defaultValue: uno-cards-v1.2
        isOptional: true
        parameterType: STRING
      images_dataset_pvc_name:
        defaultValue: images-datasets-pvc
        isOptional: true
        parameterType: STRING
      images_dataset_pvc_size_in_gi:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      images_dataset_yaml:
        defaultValue: data.yaml
        isOptional: true
        parameterType: STRING
      images_datasets_root_folder:
        defaultValue: datasets
        isOptional: true
        parameterType: STRING
      map75_threshold:
        defaultValue: 0.9
        isOptional: true
        parameterType: NUMBER_DOUBLE
      model_name:
        defaultValue: yolov8n
        isOptional: true
        parameterType: STRING
      models_root_folder:
        defaultValue: models
        isOptional: true
        parameterType: STRING
      n_trials:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      run_name:
        defaultValue: uno-cards
        isOptional: true
        parameterType: STRING
      tracking_uri:
        defaultValue: http://mlflow-server:8080
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      train-model-optuna-results_output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.8.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-get-images-dataset:
          pvcMount:
          - componentInputParameter: images_dataset_pvc_name
            mountPath: /opt/app-root/src
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-datasets
        exec-train-model-optuna:
          pvcMount:
          - componentInputParameter: images_dataset_pvc_name
            mountPath: /opt/app-root/src
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-datasets
        exec-upload-model:
          pvcMount:
          - componentInputParameter: images_dataset_pvc_name
            mountPath: /opt/app-root/src
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            secretName: aws-connection-models
