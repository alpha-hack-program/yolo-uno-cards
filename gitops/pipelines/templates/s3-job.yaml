---
apiVersion: batch/v1
kind: Job
metadata:
  name: s3-job
  namespace: "{{ .Values.dataScienceProjectNamespace }}"
  annotations:
    argocd.argoproj.io/sync-wave: "2"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  selector: {}
  backoffLimit: 10
  template:
    spec:
      containers:
        - args:
            - -ec
            - |-
              pip install boto3
              python <<EOF
              import boto3
              import os
              from botocore.exceptions import ClientError

              def create_s3_buckets():
                  bucket_names = '{{ .Values.s3.config.awsS3BucketList }}'

                  if not bucket_names:
                      print("Error: No S3 bucket names found.")
                      return
                  
                  bucket_names_list = bucket_names.split(',')

                  if not bucket_names_list:
                      print("Error: No valid bucket names provided.")
                      return
                  
                  aws_access_key_id = '{{ .Values.s3.connection.awsAccessKeyId }}'
                  aws_secret_access_key = '{{ .Values.s3.connection.awsSecretAccessKey }}'

                  if not aws_access_key_id or not aws_secret_access_key:
                      print("Error: AWS credentials not found.")
                      return

                  s3 = boto3.client(
                      's3', 
                      aws_access_key_id=aws_access_key_id,
                      aws_secret_access_key=aws_secret_access_key,
                      endpoint_url='{{ .Values.s3.connection.awsS3Endpoint }}'
                  )
                  
                  for bucket_name in bucket_names_list:
                      try:
                          s3.create_bucket(Bucket=bucket_name)
                          print(f"Bucket '{bucket_name}' successfully created")
                      
                      except ClientError as e:
                          print(f"Error creating the bucket: {e}")

              create_s3_buckets()
              EOF

          command:
            - /bin/bash
          image: "{{ .Values.images.python3 }}"
          imagePullPolicy: Always
          name: s3-job
      restartPolicy: Never
